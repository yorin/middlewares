{"name":"Middlewares","tagline":"middlewares tools for Scrapy ","body":"randomproxy.py middleware for Scrapy (http://scrapy.org/)\r\n========================================================\r\n\r\nProcesses Scrapy requests using a random proxy from list to avoid IP ban and\r\nimprove crawling speed.\r\n\r\nGet your proxy list from sites like http://www.hidemyass.com/ (copy-paste into text file\r\nand reformat to http://host:port format)\r\n\r\nsettings.py\r\n-----------\r\n\r\n    # Retry many times since proxies often fail\r\n    RETRY_TIMES = 10\r\n    # Retry on most error codes since proxies fail for different reasons\r\n    RETRY_HTTP_CODES = [500, 503, 504, 400, 403, 404, 408]\r\n\r\n    DOWNLOADER_MIDDLEWARES = {\r\n        'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 90,\r\n        # Fix path to this module\r\n        'yourspider.randomproxy.RandomProxy': 100,\r\n        'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110,\r\n    }\r\n\r\n    # Proxy list containing entries like\r\n    # http://host1:port\r\n    # http://username:password@host2:port\r\n    # http://host3:port\r\n    # ...\r\n    PROXY_LIST = '/path/to/proxy/list.txt'\r\n\r\n\r\nYour spider\r\n-----------\r\n\r\nIn each callback ensure that proxy /really/ returned your target page by\r\nchecking for site logo or some other significant element.\r\nIf not - retry request with dont_filter=True\r\n\r\n    if not hxs.select('//get/site/logo'):\r\n        yield Request(url=response.url, dont_filter=True)  \r\n  \r\n  \r\nrandomuseragent.py middleware for Scrapy (http://scrapy.org/)\r\n============================================================\r\n\r\nProcesses Scrapy provides random useragents from list to avoid ban.\r\n\r\nThe default user_agent_list composes chrome,I E,firefox,Mozilla,opera,netscape  \r\nfor more user agent strings,you can find it in http://www.useragentstring.com/pages/useragentstring.php  \r\n\r\nsettings.py\r\n-----------\r\n    DOWNLOADER_MIDDLEWARES = {\r\n        'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware' : None,\r\n        'Crawler.comm.rotate_useragent.RotateUserAgentMiddleware' :400\r\n    }\r\n    Note: Don't use 'USER_AGENT' in settings.py and add the following into it.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}